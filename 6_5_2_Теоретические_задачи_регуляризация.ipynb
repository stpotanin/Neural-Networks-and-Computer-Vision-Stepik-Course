{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/FuJZ1GDwbP3P0woMI0iI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/241397/step/2\n",
        "\n",
        "http://www.machinelearning.ru/wiki/images/d/d5/MOMO17_Seminar1.pdf\n",
        "\n",
        "http://www.machinelearning.ru/wiki/images/5/50/MOMO17_Seminar2.pdf\n",
        "\n",
        "http://www.machinelearning.ru/wiki/images/0/08/MOMO17_Homework1.pdf"
      ],
      "metadata": {
        "id": "BBqW8Shovx_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение\n",
        "Дана функция потерь:\n",
        "\n",
        "$$\n",
        "L^* = \\|Xa - y\\|_2^2 + \\lambda \\|a\\|_2^2.\n",
        "$$\n",
        "\n",
        "Запишем производную $ L^* $ по вектору $ a $.\n",
        "\n",
        "### 1. Первое слагаемое $ \\|Xa - y\\|_2^2 $:\n",
        "\n",
        "$$\n",
        "\\|Xa - y\\|_2^2 = (Xa - y)^\\top (Xa - y).\n",
        "$$\n",
        "\n",
        "Найдем градиент этого выражения по $ a $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial a} \\left( (Xa - y)^\\top (Xa - y) \\right) = 2X^\\top (Xa - y).\n",
        "$$\n",
        "\n",
        "### 2. Второе слагаемое $ \\lambda \\|a\\|_2^2 $:\n",
        "\n",
        "$$\n",
        "\\|a\\|_2^2 = a^\\top a.\n",
        "$$\n",
        "\n",
        "Градиент по $ a $ будет:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial a} \\left( \\lambda a^\\top a \\right) = 2\\lambda a.\n",
        "$$\n",
        "\n",
        "### 3. Общая производная:\n",
        "\n",
        "Теперь объединим обе производные:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L^*}{\\partial a} = 2X^\\top (Xa - y) + 2\\lambda a.\n",
        "$$\n",
        "\n",
        "### Ответ:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L^*}{\\partial a} = 2X^\\top (Xa - y) + 2\\lambda a.\n",
        "$$\n",
        "\n",
        "Можно вынести общий множитель 2:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L^*}{\\partial a} = 2 \\left( X^\\top (Xa - y) + \\lambda a \\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "h4QAW_NMIc8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 * (X.T * (X * a - y) + reg * a)"
      ],
      "metadata": {
        "id": "7V0FMaJvI4_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   λ обозначайте как reg\n",
        "*   единичную матрицу (у которой по диагонали стоят единицы, а в остальных позициях -- нули) обозначайте как `I` ( буква i)\n",
        "*   если получаются целые числа, пишите `2`, но не `2.0`\n",
        "не забывайте писать знаки умножения (`a * C`, но не `a C`)\n",
        "*   транспонирование обозначайте как `.T`\n",
        "*   можете писать как строчными, так и ЗАГЛАВНЫМИ\n",
        "\n",
        "Например, если Вы считаете, что ответ будет\n",
        "$λ2AT^BIc$ то пишите `reg * 2 * A.T * B * I * c`"
      ],
      "metadata": {
        "id": "kqgcTdgHJdLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример 1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([[1, 2], [3, 4]])  # Матрица 2x2\n",
        "a = np.array([[0.5], [1.0]])    # Вектор 2x1\n",
        "y = np.array([[1.0], [2.0]])    # Вектор 2x1\n",
        "λ = 0.1                         # Параметр регуляризации\n",
        "\n",
        "# Вычисление функции потерь\n",
        "def compute_loss_derivative(X, a, y, λ):\n",
        "    # Вычисляем производную L* по a\n",
        "    grad = 2 * (X.T @ (X @ a - y) + λ * a)\n",
        "    return grad\n",
        "\n",
        "# Рассчитываем производную\n",
        "grad = compute_loss_derivative(X, a, y, λ)\n",
        "\n",
        "print(\"Производная функции потерь по a:\")\n",
        "print(grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDo7Bl0bLrYY",
        "outputId": "68a50040-eee7-4fd1-8a17-d4ab1dd0842d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Производная функции потерь по a:\n",
            "[[24.1]\n",
            " [34.2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "X — это матрица признаков.\n",
        "a — это вектор параметров.\n",
        "y — это целевой вектор.\n",
        "λ — это параметр регуляризации.\n",
        "Программа вычисляет производную функции потерь $L'$ по $a$\n",
        "для небольшого примера с матрицами $X$, $a$ и $y$."
      ],
      "metadata": {
        "id": "iq8r2DCMMqnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Пример 2\n",
        "import numpy as np\n",
        "\n",
        "# Зададим небольшие матрицы для примера\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])  # матрица независимых переменных (признаков) 3x2,\n",
        "    # где каждая строка соответствует одному объекту, а каждый столбец — одному признаку\n",
        "y = np.array([1, 2, 3])  # это вектор зависимых переменных (целевых значений или меток) 3x1,\n",
        "    # где каждое значение соответствует зависимой переменной для соответствующей строки из X,\n",
        "    # размерность y — n×1, где n  — количество наблюдений (в нашем примере их 3).\n",
        "a = np.array([0.5, 1.0])  # это вектор параметров модели, которые мы хотим обучить.\n",
        "    # размерность a — m×1, где m — количество признаков (в нашем примере их 2).\n",
        "    # Модель пытается найти такие параметры a, чтобы минимизировать функцию потерь,\n",
        "    # т.е. приблизить предсказанные значения Xa к истинным значениям y.\n",
        "reg = 0.1  # регуляризационный параметр λ\n",
        "\n",
        "# Вычисление функции потерь\n",
        "def loss_derivative(X, y, a, reg):\n",
        "    # Вычисляем производную функции потерь\n",
        "    gradient = 2 * (X.T @ (X @ a - y) + reg * a)\n",
        "    return gradient\n",
        "\n",
        "# Вычислим производную\n",
        "grad = loss_derivative(X, y, a, reg)\n",
        "\n",
        "# Выведем результат\n",
        "print(\"Производная функции потерь по a:\")\n",
        "print(all(grad == [79.1, 100.2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvpHbrIAL1MR",
        "outputId": "8b39f604-453e-4764-fa6a-b2c749142c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Производная функции потерь по a:\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Объяснение:\n",
        "\n",
        "- Мы создаем матрицу $ X $ (3x2), вектор $ y $ (3x1) и вектор параметров $ a $ (2x1).\n",
        "- Регуляризационный параметр $ \\lambda $ задается равным 0.1.\n",
        "- Функция `loss_derivative` реализует производную, которую мы вычислили ранее: $ 2(X^\\top (Xa - y) + \\lambda a) $.\n",
        "- В конце программа выводит градиент функции потерь по вектору $ a $.\n",
        "\n",
        "### Пример вывода:\n",
        "\n",
        "```plaintext\n",
        "Производная функции потерь по a:\n",
        "[30.2 41.6]\n",
        "```\n",
        "\n",
        "Этот результат — численный градиент функции потерь по вектору $ a $."
      ],
      "metadata": {
        "id": "sQGJly3hM3k1"
      }
    }
  ]
}