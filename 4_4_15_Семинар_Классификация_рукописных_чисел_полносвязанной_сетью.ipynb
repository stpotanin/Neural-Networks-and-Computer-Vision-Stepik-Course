{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWbBmyFIBIrMvuP13rlDcW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/211876/step/15\n",
        "\n",
        "[Знакомство с трансформерами. Часть 1](https://habr.com/ru/companies/wunderfund/articles/592231/)"
      ],
      "metadata": {
        "id": "riCVTOz35jVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Сперва создадим тензор x:\n",
        "x = torch.tensor([[10., 20.]])\n",
        "\n",
        "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
        "fc = torch.nn.Linear(2, 3)\n",
        "\n",
        "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
        "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
        "\n",
        "# Давайте проставим свои значения в веса и bias'ы:\n",
        "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
        "fc.weight.data = w\n",
        "\n",
        "b = torch.tensor([[31., 32., 33.]])\n",
        "fc.bias.data = b\n",
        "\n",
        "# Получим выход fc-слоя:\n",
        "fc_out = fc(x)\n",
        "# Просуммируем выход fc-слоя, чтобы получить скаляр:\n",
        "fc_out_summed = fc_out.sum()\n",
        "\n",
        "# Посчитаем градиенты формулы fc_out_summed:\n",
        "fc_out_summed.backward()\n",
        "weight_grad = fc.weight.grad\n",
        "bias_grad = fc.bias.grad\n",
        "\n",
        "# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\n",
        "# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "# Получим выход нашей формулы:\n",
        "our_formula = ... # SUM{x * w^T + b}\n",
        "\n",
        "# Сделайте backward для нашей формулы:\n",
        "...\n",
        "\n",
        "# Проверка осуществляется автоматически, вызовом функций:\n",
        "#print('fc_weight_grad:', weight_grad)\n",
        "#print('our_weight_grad:', w.grad)\n",
        "#print('fc_bias_grad:', bias_grad)\n",
        "#print('out_bias_grad:', b.grad)\n",
        "# (раскомментируйте, если работаете над задачей локально)import torch\n",
        "\n",
        "# Сперва создадим тензор x:\n",
        "x = torch.tensor([[10., 20.]])\n",
        "\n",
        "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
        "fc = torch.nn.Linear(2, 3)\n",
        "\n",
        "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
        "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
        "\n",
        "# Давайте проставим свои значения в веса и bias'ы:\n",
        "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
        "fc.weight.data = w\n",
        "\n",
        "b = torch.tensor([[31., 32., 33.]])\n",
        "fc.bias.data = b\n",
        "\n",
        "# Получим выход fc-слоя:\n",
        "fc_out = fc(x)\n",
        "# Просуммируем выход fc-слоя, чтобы получить скаляр:\n",
        "fc_out_summed = fc_out.sum()\n",
        "\n",
        "# Посчитаем градиенты формулы fc_out_summed:\n",
        "fc_out_summed.backward()\n",
        "weight_grad = fc.weight.grad\n",
        "bias_grad = fc.bias.grad\n",
        "\n",
        "# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\n",
        "# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "# Получим выход нашей формулы:\n",
        "our_formula = (x @ torch.t(w) + b).sum().backward() # SUM{x * w^T + b}\n",
        "\n",
        "# Сделайте backward для нашей формулы:\n",
        "# our_formula\n",
        "\n",
        "# Проверка осуществляется автоматически, вызовом функций:\n",
        "print('fc_weight_grad:', weight_grad)\n",
        "print('our_weight_grad:', w.grad)\n",
        "print('fc_bias_grad:', bias_grad)\n",
        "print('out_bias_grad:', b.grad)\n",
        "# (раскомментируйте, если работаете над задачей локально)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGvVrarONEuj",
        "outputId": "0e5c0969-7639-4921-d47a-0e16c9256d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc_weight_grad: tensor([[10., 20.],\n",
            "        [10., 20.],\n",
            "        [10., 20.]])\n",
            "our_weight_grad: tensor([[10., 20.],\n",
            "        [10., 20.],\n",
            "        [10., 20.]])\n",
            "fc_bias_grad: tensor([[1., 1., 1.]])\n",
            "out_bias_grad: tensor([[1., 1., 1.]])\n"
          ]
        }
      ]
    }
  ]
}